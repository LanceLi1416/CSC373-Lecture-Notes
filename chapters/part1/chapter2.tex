\chapter{Divide and Conquer}

\textit{Veni, vidi, vici.} 
\begin{flushright}
    --- Gaius Julius Caesar
\end{flushright}

\section{Introduction}

\term{Divide and Conquer}\index{Divide and Conquer} is a general algorithm design paradigm

\begin{listu}
    \item \textbf{Divide} the problem into smaller subproblems of the same type.
    \item \textbf{Conquer} each subproblems recursively and independently.
    \item \textbf{Combine} solutions from subproblems and/or solve remaining part of the original problem.
\end{listu}

\begin{example}[Merge Sort]
    \textsc{Merge-Sort} is a sorting algorithm that uses the divide and conquer paradigm.

    \begin{listu}
        \item \textbf{Divide}: Split the array into two halves
        \item \textbf{Conquer}: Sort the two halves recursively
        \item \textbf{Combine}: Merge the two sorted halves into a sorted array
    \end{listu}
\end{example}

\begin{remark}
    When analyzing divide and conquer algorithms, \bred{constants} matter due to the recursive nature of the algorithm.
\end{remark}

\subsection{Merge Sort}

Merge sort is a sorting algorithm that uses the divide and conquer paradigm. It divides the array into two halves, sorts the two halves recursively, and merges the two sorted halves into a sorted array.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{Merge-Sort}{$A$}
            \If{$|A| \le 2$}
                \State \Return \Call{BruteForceSort}{$A$}
            \EndIf
            \State $m \gets \lfloor |A| / 2 \rfloor$
            \State $L \gets \Call{Merge-Sort}{A[1 \dots m]}$
            \State $R \gets \Call{Merge-Sort}{A[m+1 \dots |A|]}$
            \State \Return $\Call{Merge}{L, R}$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{claim}
    Two arrays of length $m$ that are sorted can be combined into a sorted string in $\mathcal{O}(m)$ time.
\end{claim}

\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{Merge}{$L, R$}
            \State $i \gets 1$
            \State $j \gets 1$
            \State $A \gets \emptyset$
            \While{$i \le |L|$ and $j \le |R|$}
                \If{$L[i] \le R[j]$}
                    \State $A \gets A \cup \{L[i]\}$
                    \State $i \gets i + 1$
                \Else
                    \State $A \gets A \cup \{R[j]\}$
                    \State $j \gets j + 1$
                \EndIf
            \EndWhile
            \State \Return $A \cup L[i \dots] \cup R[j \dots]$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{center}
    \tikzexternalenable
    \begin{tikzpicture}
        \node (root) at (0, 0) {$n$};
        \node (l) at (-2, -1)  {$\displaystyle \frac{n}{2^1}$};
        \node (r) at (2, -1)   {$\displaystyle \frac{n}{2^1}$};
        \node (ll) at (-3, -2) {$\displaystyle \frac{n}{2^2}$};
        \node (lr) at (-1, -2) {$\displaystyle \frac{n}{2^2}$};
        \node (rl) at (1, -2)  {$\displaystyle \frac{n}{2^2}$};
        \node (rr) at (3, -2)  {$\displaystyle \frac{n}{2^2}$};
        % dots below the leaves
        \node (llDots) at (-3, -3) {$\vdots$};
        \node (lrDots) at (-1, -3) {$\vdots$};
        \node (rlDots) at (1, -3) {$\vdots$};
        \node (rrDots) at (3, -3) {$\vdots$};


        \draw (root) -- (l);
        \draw (root) -- (r);
        \draw (l) -- (ll);
        \draw (l) -- (lr);
        \draw (r) -- (rl);
        \draw (r) -- (rr);
    \end{tikzpicture}
    \tikzexternaldisable
\end{center}

To compute the cost of \textsc{Merge-Sort}, we need to compute the cost of \textsc{Merge} and the cost of the levels. 

\begin{claim}
    The cost of the levels is \[
        \left( \frac{n}{2} \right) \cdot \mathcal{O}(2^2) = \mathcal{O}(n)
    \]
\end{claim}

Indeed, in each level $j$, there are $2^j$ subproblems of size $\frac{n}{2^j}$, and the cost of each subproblem is $\mathcal{O}(2^j)$. Thus, the cost of each level is \[
    \left( \frac{n}{2^j} \right) \cdot \mathcal{O}(2^j) = \mathcal{O}(n).
\]

Then, the cost of \textsc{Merge-Sort} is \[
    \sum_{j=1}^{\log_2 n - 1} \left( \frac{n}{2^j} \right) \cdot \mathcal{O}(2^j) = \mathcal{O}(n \log n)
\]

\begin{claim}
    \textsc{Merge-Sort} is correct.
\end{claim}

% \begin{proof}
%     Proof by contradiction. 

%     Assume that \textsc{Merge-Sort} is not correct. 

%     % TODO
%     TODO: prove on a case by case basis
% \end{proof}

\begin{proof}
    By induction on the number of iterations of \textsc{Merge-Sort}.

    \begin{listu}
        \item \textbf{Base case}: $s(2)$
        
        \textsc{BruteForceSort} is correct by construction.

        \item \textbf{Induction Steps}
        
        Assume \textsc{Merge-Sort} is correct for any array $s$ of length $L \ge 2$. 

        Without loss of generality, assume $L$ is a power of 2. For the other cases, some extra work is needed.
        
        \begin{center}
            \tikzexternalenable
            \begin{tikzpicture}
                \node (root) at (0, 0) {};
                \filldraw (root) circle (2pt);
                \node (l) at (-2, -1)  {$s_1$ with $L$ elements};
                \node (r) at (2, -1)   {$s_2$ with $L$ elements};
            
                \draw (root) -- (l);
                \draw (root) -- (r);
            \end{tikzpicture}
            \tikzexternaldisable
        \end{center}

        If \textsc{Merge-Sort} is correct, then $s_1$ and $s_2$ are sorted.

        For $j = 1$ to $L$, compare $s_1[j]$ to $s_2[j]$ and insert if $s_1[j] \ge s_2[j]$.

        The algorithm guarantees that inserted $s_1[j] \ge s_2[j]$. Thus, insertion is correct.

        This implies that, in cases of mistake, then the order must have been wrong to start with. 

        This is a contradiction, as we assumed that \textsc{Merge-Sort} is correct for $L$ elements.
    \end{listu}

    Thus, \textsc{Merge-Sort} is correct. 
\end{proof}

\subsection{Counting Inversions}

\begin{listu}
    \item \textbf{Problem}
    
    Given an array $a$ of length $n$, count the number of pairs $(i, j)$ such that $i < j$ but $a[i] > a[j]$.

    \item \textbf{Applications}
    
    \begin{listu}
        \item Voting theory
        \item Collaborative filtering 
        \item Measuring the ``sortedness'' of an array
        \item Seneitivity analysis of Google's ranking function 
        \item \dots
    \end{listu}
\end{listu}

\begin{definition}[Inversion]\index{Inversion}\label{def:inversion}
    An \term{inversion} is a pair $(i, j)$ such that $i < j$ but $a[i] > a[j]$.
\end{definition}

The brute force algorithm is to check all pairs $(i, j)$ and count the number of inversions. This is $\mathcal{O}(n^2)$. We can do better by using the divide and conquer paradigm.

\begin{listu}
    \item \textbf{Divide}: Split the array into two equal halves $x$ and $y$
    \item \textbf{Conquer}: Count the number of inversions in the two halves recursively
    \item \textbf{Combine}:
    \begin{listu}
        \item \itblue{Solve}: Count the number of inversions where $i \in x$ and $j \in y$
        \item \itblue{Merge}: Add the three counts together
    \end{listu}
\end{listu}

\begin{algorithm}    
    \begin{algorithmic}[1]
        \Function{Sort-And-Count}{$L$}
            \If{$|A| \le 1$}
                \State \Return $(0, L)$
            \EndIf
            \State
            \State \textsc{\color{primary}Divide} the list into two halves $A$ and $B$
            \State $(r_A, A) \gets \Call{Sort-And-Count}{A}$
            \State $(r_B, B) \gets \Call{Sort-And-Count}{B}$
            \State $(r_{AB}, L') \gets \Call{Merge-And-Count}{A, B}$
            \State
            \State \Return $(r_A + r_B + r_{AB}, L')$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{center}
    \includegraphics[width=0.5\linewidth]{figures/counting-inversions.png}
\end{center}

Counting inversions $i \in x$ and $j \in y$ is done by merging the two sorted halves.

\begin{listu}
    \item Scan $x$ and $y$ in parallel from left to right
    \item If $x[i] \le y[j]$, then $x[i]$ is not an inversion
    If $x[i] > y[j]$, then $x[i]$ is an inversion with all elements in $y$ that have not been scanned yet
    \item Append the smaller element to the output array
\end{listu}

\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{Merge-And-Count}{$L, R$}
            \State $i \gets 1$
            \State $j \gets 1$
            \State $A \gets \emptyset$
            \State $r_{LR} \gets 0$
            \While{$i \le |L|$ and $j \le |R|$}
                \If{$L[i] \le R[j]$}
                    \State $A \gets A \cup \{L[i]\}$
                    \State $i \gets i + 1$
                \Else
                    \State $A \gets A \cup \{R[j]\}$
                    \State $j \gets j + 1$
                    \State $r_{LR} \gets r_{LR} + |L| - i + 1$
                \EndIf
            \EndWhile
            \State \Return $(A \cup L[i \dots] \cup R[j \dots], r_{LR})$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

To formally prove correctness of \textsc{SortAndCount}, we can induce on the size of the array, $n$. 

To analyze the running time of \textsc{SortAndCount}, 

\begin{listu}
    \item Suppose $T(n)$ is the worst-case running time for inputs of size $n$
    \item Our algorithm satisfies $T(n) \le 2T(\frac{n}{2}) + \mathcal{O}(n)$
    \item Master theorem says this is $T(n) = \mathcal{O}(n \log n)$
\end{listu}

\section{Master Theorem}

\begin{theorem}[Master Theorem]\index{Master Theorem}
    Let $a \ge 1$ and $b > 1$ be constants, let $f(n)$ be a function, and let $T(n)$ be defined on the nonnegative integers by the recurrence \[
        T(n) \le a \cdot T\left( \frac{n}{b} \right) + f(n)
    \]

    where we interpret $n/b$ to mean either $\lfloor n/b \rfloor$ or $\lceil n/b \rceil$. 
    
    Let $d = \log_b a$. Then, $T(n)$ has the following asymptotic bounds:
    \begin{enumerate}
        \item If $f(n) = \mathcal{O}(n^{d - \epsilon})$ for some constant $\epsilon > 0$, then $T(n) = \mathcal{O}(n^d)$.

        This is the \bred{merge heavy} case. The cost of merging dominates the cost of recursion.

        \item If $f(n) = \mathcal{O}(n^d \log^k n)$, then $T(n) = \mathcal{O}(n^d \log^{k+1} n)$.

        This is the \bred{balanced} case. The cost of merging and recursion are the same.

        \item If $f(n) = \mathcal{O}(n^{d + \epsilon})$ for some constant $\epsilon > 0$, then $T(n) = \mathcal{O}(f(n))$.

        This is the \bred{leaf (recursion) heavy} case. The cost of recursion dominates the cost of merging.
    \end{enumerate}
\end{theorem}

\begin{figure}[ht!]
    \centering

    \includegraphics[width=\linewidth]{figures/master-theorem.png}
\end{figure}

\subsection{Closest Pair}

\begin{listu}
    \item \textbf{Problem}
    
    Given $n$ points of the form $(x_i, y_i)$ in the plane, find the closest pair of points.

    \item \textbf{Applications}
    
    \begin{listu}
        \item Basic primitive in graphics and computer vision
        \item Geographic information systems, molecular modeling, air traffic control
        \item Special case of nearest neighbor
    \end{listu}

    \item \textbf{Brute force} is $\mathcal{O}(n^2)$.
\end{listu}

We can use the divide and conquer paradigm to solve this problem.

\begin{listu}
    \item \textbf{Divide}: Split the points into two equal halves by drawing a vertical line $L$ through the median $x$-coordinate

    \begin{center} \includegraphics[width=0.55\linewidth]{figures/Closest Pair Divide.png} \end{center}

    \item \textbf{Conquer}: Find the closest pair of points in each half recursively

    \item \textbf{Combine}: Find the closest pair of points with one point in each half

    We can restrict our attention to points within $\delta$ of $L$ on each side, where $\delta = $ best of the solutions within the two halves. 

    \begin{center} \includegraphics[width=0.55\linewidth]{figures/Closest Pair Conquer.png} \end{center}

    \begin{listu}
        \item Only need to look at points within $\delta$ of $L$ on each side
        \item Sort points on the strip by $y$ coordinate
        \item Only need to check each point with next 11 points in sorted list
    \end{listu}

    \item Return the best of 3 solutions
\end{listu}

\begin{remark}
    \begin{minipage}[t]{0.55\linewidth}
        We chose the number $11$ on purpose.

        \begin{claim}
            If two points are at least $12$ positions apart in the sorted list, their distance is at least $\delta$.
        \end{claim}

        \begin{proof}
            {~~~}

            \begin{listu}
                \item No two points lie in the same $\frac{\delta}{2} \times \delta$ rectangle.

                \item Two points that are more than two rows apart are at distance $\delta$.
            \end{listu}
        \end{proof}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\linewidth}
        \begin{center} \raisebox{-0.9\height}{ \includegraphics[width=0.9\linewidth]{figures/Closest Pair Grid.png} } \end{center}
    \end{minipage}
\end{remark}

Let $T(n)$ be the worst-case running time of the algorithm. To analyze the Running time for the combine operation, 
\begin{listu}
    \item Finding points on the strip is $\mathcal{O}(n)$
    \item Sorting points on the strip by their $y$-coordinate is $\mathcal{O}(n \log n)$
    \item Testing each point against 11 points is $\mathcal{O}(n)$
\end{listu}

Thus, the total running running time is \[
    T(n) \le 2T\left( \frac{n}{2} \right) + \mathcal{O}(n \log n)
\] 

By the master theorem, this yields $T(n) = \mathcal{O}(n \log^2 n)$.

\subsection{Multiplication Algorithms}

\subsubsection{Karatsuba's Algorithm}\index{Karatsuba's Algorithm}\label{subsubsec:karatsuba}

\begin{listu}
    \item \textbf{Problem}
    
    Given two $n$-bit integers $x$ and $y$, compute their product $xy$.

    \item \textbf{Applications}
    
    \begin{listu}
        \item Multiplying large integers
        \item Multiplying large polynomials
        \item Multiplying large matrices
    \end{listu}

    \item \textbf{Brute force} is $\mathcal{O}(n^2)$.
\end{listu}

Karatsuba's observed that we can divide each integer into two halves, \[
    x = x_1 \cdot 10^{\frac{n}{2}} + x_2  \qquad y = y_1 \cdot 10^{\frac{n}{2}} + y_2
\] and then \[
    xy = (x_1y_1) \cdot 10^n + (x_1y_2 + x_2y_1) \cdot 10^{\frac{n}{2}} + x_2y_2
\] so four $n/2$-bit integer multiplications can be replaced by three: \[
    x_1y_2 + x_2y_1 = (x_1 + x_2)(y_1 + y_2) - x_1y_1 - x_2y_2
\] This would give a running time of \[
    T(n) \le 3T\left( \frac{n}{2} \right) + \mathcal{O}(n) \implies T(n) = \mathcal{O}(n^{\log_2 3}) \approx \mathcal{O}(n^{1.59})
\]

\subsubsection{Strassen's Algorithm}

Strassen's algorithm is a generalization of \hyperref[subsubsec:karatsuba]{Karatsuba's algorithm} to design a fast algorithm for multiplying two $n \times n$ matrices.

\begin{listu}
    \item We call $n$ the ``size'' of the problem \[
        \begin{bmatrix}
            C_{11} & C_{12} \\
            C_{21} & C_{22}
        \end{bmatrix} = \begin{bmatrix}
            A_{11} & A_{12} \\
            A_{21} & A_{22}
        \end{bmatrix} \begin{bmatrix}
            B_{11} & B_{12} \\
            B_{21} & B_{22}
        \end{bmatrix}
    \]

    \item Nat\"ively, this requires $2^3 = 8$ matrix multiplications of size $\frac{n}{2}$. 
    
    \item Strassen's algorithm reduces this to $7$ matrix multiplications instead of $8$. \[
        T(n) \le 7T\left( \frac{n}{2} \right) + \mathcal{O}(n^2) \implies T(n) = \mathcal{O}(n^{\log_2 7}) \approx \mathcal{O}(n^{2.81})
    \]
\end{listu}
\vspace{-2em}
\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{Strassen}{$n, A, B$}
        \If{$n = 1$}
            % \State \Return $A \times B$
            \Return $A \times B$
        \EndIf

        {~~~}

        \State Partition $A$ and $B$ into $2 \times 2$ block matrices
        \State $P_1 \gets \Call{Strassen}{\frac{n}{2}, A_{11}, (B_{12} - B_{22})}$
        \State $P_2 \gets \Call{Strassen}{\frac{n}{2}, (A_{11} + A_{12}), B_{22}}$
        \State $P_3 \gets \Call{Strassen}{\frac{n}{2}, (A_{21} + A_{22}), B_{11}}$
        \State $P_4 \gets \Call{Strassen}{\frac{n}{2}, A_{22}, (B_{21} - B_{11})}$
        \State $P_5 \gets \Call{Strassen}{\frac{n}{2}, (A_{11} + A_{22}), (B_{11} + B_{22})}$
        \State $P_6 \gets \Call{Strassen}{\frac{n}{2}, (A_{12} - A_{22}), (B_{21} + B_{22})}$
        \State $P_7 \gets \Call{Strassen}{\frac{n}{2}, (A_{11} - A_{21}), (B_{11} + B_{12})}$

        {~~~}

        \State $C_{11} \gets P_5 + P_4 - P_2 + P_6$
        \State $C_{12} \gets P_1 + P_2$
        \State $C_{21} \gets P_3 + P_4$
        \State $C_{22} \gets P_1 + P_5 - P_3 - P_7$

        {~~~}

        \State \Return $C$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Median and Selection}

\begin{listu}
    \item \textbf{Selection}
    
    \begin{listu}
        \item Given an array $A$ of $n$ comparable elements, find the $k$-th smallest element in $A$. 
        \item $k = 1$ is the minimum, $k = n$ is the maximum, and $k = \floor{\frac{n}{2}}$ is the median. 
        \item The running time is $\mathcal{O}(n)$ for minimum and maximum. 
    \end{listu}

    \item \textbf{$k$-Selection}

    \begin{listu}
        \item $\bigo{nk}$ by modifying bubble sort.
        \item $\bigo{n \log n}$ by sorting. 
        \item $\bigo{n + k \log n}$ by using a min-heap.
        \item $\bigo{k + n \log k}$ by using a max-heap.
        \item $\bigo{n}$ by using the divide and conquer paradigm.
    \end{listu}
\end{listu}

\subsubsection{QuickSelect}

\begin{listu}
    \item \textbf{Divide}: Pick a pivot $p$ at random from $A$

    \item \textbf{Conquer}: Partition $A$ into two sub-arrays 
    
    \begin{listu}
        \item $A_{less}$ contains all elements less than $p$
        \item $A_{more}$ contains all elements greater than $p$
    \end{listu}

    \item \textbf{Combine}: 
    \begin{listu}
        \item If $|A_{less}| \ge k$, return $k$-th smallest element in $A_{less}$
        \item Otherwise, return $(k - |A_{less}|)$-th smallest element in $A_{more}$
    \end{listu}
\end{listu}

However, this algorithm is not guaranteed to be $\mathcal{O}(n)$, as the pivot may be the largest or smallest element in the array. If pivot is close to the min or the max, then we basically get $T(n) \le T(n - 1) + \bigo{n}$, which is $\bigo{n^2}$. We want to reduce $n - 1$ to a fraction of $n$. 

% TODO